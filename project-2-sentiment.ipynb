{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3238154,"sourceType":"datasetVersion","datasetId":1962861},{"sourceId":13331512,"sourceType":"datasetVersion","datasetId":8452437},{"sourceId":13331530,"sourceType":"datasetVersion","datasetId":8452452},{"sourceId":13412535,"sourceType":"datasetVersion","datasetId":8512329},{"sourceId":13412550,"sourceType":"datasetVersion","datasetId":8512337}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Task 1\nimport pandas as pd\nimport re\nimport os\nfrom tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers\nfrom tokenizers.normalizers import Lowercase, NFD, StripAccents\nfrom sklearn.model_selection import train_test_split\n\n# Load Dataset\n\ndata_path = \"/kaggle/input/empathetic-dialogues-facebook-ai/emotion-emotion_69k.csv\"\ndf = pd.read_csv(data_path)\n\nprint(\"Columns:\", df.columns.tolist())\nprint(\"Total rows:\", len(df))\ndf = df.rename(columns=str.strip)  \n\n\ndef normalize_text(text):\n    text = str(text).lower().strip()\n    text = re.sub(r\"\\s+\", \" \", text)  # normalize whitespace\n    text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)  # space around punctuation\n    text = re.sub(r\"[^a-zA-Z?.!,¿']+\", \" \", text)  # remove non-alphabetic\n    return text.strip()\n\nfor col in [\"Situation\", \"empathetic_dialogues\", \"labels\"]:\n    df[col] = df[col].apply(normalize_text)\n\ntexts = []\nfor _, row in df.iterrows():\n    input_text = f\"<emotion_{row['emotion']}> <bos> {row['Situation']} <sep> {row['empathetic_dialogues']}\"\n    target_text = f\"<bos> {row['labels']} <eos>\"\n    texts.append(input_text)\n    texts.append(target_text)\n\ntokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\ntokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\ntokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n\nspecial_tokens = [\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\", \"<sep>\"] + [\n    f\"<emotion_{emo}>\" for emo in df[\"emotion\"].unique()\n]\n\ntrainer = trainers.BpeTrainer(vocab_size=2000, special_tokens=special_tokens)\ntokenizer.train_from_iterator(texts, trainer=trainer)\n\nos.makedirs(\"tokenizer\", exist_ok=True)\ntokenizer.save(\"tokenizer/empathetic_tokenizer.json\")\nprint(\"✅ Tokenizer trained and saved at tokenizer/empathetic_tokenizer.json\")\nprint(\"Vocab size:\", tokenizer.get_vocab_size())\n\ntrain_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\nval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n\nprint(f\"Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")\n\ntrain_df.to_csv(\"train.csv\", index=False)\nval_df.to_csv(\"val.csv\", index=False)\ntest_df.to_csv(\"test.csv\", index=False)\n\nprint(\"✅ Data splits saved: train.csv, val.csv, test.csv\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-17T13:24:32.898270Z","iopub.execute_input":"2025-10-17T13:24:32.898454Z","iopub.status.idle":"2025-10-17T13:24:44.180286Z","shell.execute_reply.started":"2025-10-17T13:24:32.898438Z","shell.execute_reply":"2025-10-17T13:24:44.179539Z"}},"outputs":[{"name":"stdout","text":"Columns: ['Unnamed: 0', 'Situation', 'emotion', 'empathetic_dialogues', 'labels', 'Unnamed: 5', 'Unnamed: 6']\nTotal rows: 64636\n\n\n\n✅ Tokenizer trained and saved at tokenizer/empathetic_tokenizer.json\nVocab size: 2000\nTrain: 51708 | Val: 6464 | Test: 6464\n✅ Data splits saved: train.csv, val.csv, test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Task 2\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom tokenizers import Tokenizer\n\ntrain_df = pd.read_csv(\"/kaggle/working/train.csv\")\nval_df = pd.read_csv(\"/kaggle/working/val.csv\")\ntest_df = pd.read_csv(\"/kaggle/working/test.csv\")\n\nfor df in [train_df, val_df, test_df]:\n    df.columns = df.columns.str.strip()\n    df.fillna(\"\", inplace=True)\n\nprint(f\"✅ Data Loaded — Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")\n\n\ntokenizer = Tokenizer.from_file(\"/kaggle/working/tokenizer/empathetic_tokenizer.json\")\npad_id = tokenizer.token_to_id(\"<pad>\")\nbos_id = tokenizer.token_to_id(\"<bos>\")\neos_id = tokenizer.token_to_id(\"<eos>\")\nsep_token = \"<sep>\"\n\nprint(f\"✅ Tokenizer loaded — Vocab size: {tokenizer.get_vocab_size()}\")\n\ndef build_input(row):\n    \"\"\"Builds the input (X) sequence in a clear empathetic structure.\"\"\"\n    return (\n        f\"Emotion: {row['emotion']} | \"\n        f\"Situation: {row['Situation']} | \"\n        f\"Customer: {row['empathetic_dialogues']} Agent:\"\n    )\n\ndef build_target(row):\n    \"\"\"Builds the target (Y) sequence (Agent response).\"\"\"\n    return str(row[\"labels\"]).strip()\n\nfor df in [train_df, val_df, test_df]:\n    df[\"X\"] = df.apply(build_input, axis=1)\n    df[\"Y\"] = df.apply(build_target, axis=1)\n\nMAX_LEN = 128  \n\ndef encode(text, add_special_tokens=True):\n    \"\"\"Tokenizes and pads text sequences to fixed MAX_LEN.\"\"\"\n    text = str(text)\n    if add_special_tokens:\n        text = f\"<bos> {text} <eos>\"\n    tokens = tokenizer.encode(text)\n    ids = tokens.ids[:MAX_LEN]\n    if len(ids) < MAX_LEN:\n        ids += [pad_id] * (MAX_LEN - len(ids))\n    return torch.tensor(ids, dtype=torch.long)\n\nclass EmpatheticDataset(Dataset):\n    def __init__(self, df):\n        self.inputs = df[\"X\"].tolist()\n        self.targets = df[\"Y\"].tolist()\n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self, idx):\n        x_ids = encode(self.inputs[idx])\n        y_ids = encode(self.targets[idx])\n        return {\"input_ids\": x_ids, \"target_ids\": y_ids}\n\ntrain_dataset = EmpatheticDataset(train_df)\nval_dataset = EmpatheticDataset(val_df)\ntest_dataset = EmpatheticDataset(test_df)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True)\nval_loader = DataLoader(val_dataset, batch_size=32, drop_last=False)\ntest_loader = DataLoader(test_dataset, batch_size=32, drop_last=False)\n\nprint(f\"✅ Dataloaders ready — Train: {len(train_loader)} | Val: {len(val_loader)} | Test: {len(test_loader)}\")\n\nsample = next(iter(train_loader))\nidx = 0  # pick first sample in batch\nprint(\"\\n✅ Example:\")\nprint(\"Input:\", train_df['X'].iloc[idx])\nprint(\"Target:\", train_df['Y'].iloc[idx])\nprint(\"input_ids shape:\", sample[\"input_ids\"].shape)\nprint(\"target_ids shape:\", sample[\"target_ids\"].shape)\nprint(\"✅ Encoding verified and meaningful!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T13:25:39.079377Z","iopub.execute_input":"2025-10-17T13:25:39.079963Z","iopub.status.idle":"2025-10-17T13:25:43.916023Z","shell.execute_reply.started":"2025-10-17T13:25:39.079938Z","shell.execute_reply":"2025-10-17T13:25:43.915277Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_37/2329543898.py:19: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.fillna(\"\", inplace=True)\n","output_type":"stream"},{"name":"stdout","text":"✅ Data Loaded — Train: 51708 | Val: 6464 | Test: 6464\n✅ Tokenizer loaded — Vocab size: 2000\n✅ Dataloaders ready — Train: 1615 | Val: 202 | Test: 202\n\n✅ Example:\nInput: Emotion: nostalgic | Situation: i had to go buy legos for my nephew the other day . makes me miss the days when my girls were young enough to play with them . | Customer: customer were you embarrassed or what happend ? agent Agent:\nTarget: no just this feeling overcame me that my kids just have outgrown this time .\ninput_ids shape: torch.Size([32, 128])\ntarget_ids shape: torch.Size([32, 128])\n✅ Encoding verified and meaningful!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Task 3\nimport math\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom tokenizers import Tokenizer\n\ndef create_padding_mask(seq, pad_id):\n    return (seq == pad_id).unsqueeze(1).unsqueeze(2)\n\ndef create_look_ahead_mask(size):\n    return torch.triu(torch.ones((size, size), dtype=torch.bool), diagonal=1)\n\ndef scaled_dot_product_attention(q, k, v, mask=None, dropout=None):\n    dk = q.size(-1)\n    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(dk)\n    if mask is not None:\n        scores = scores.masked_fill(mask, float(\"-inf\"))\n    attn = F.softmax(scores, dim=-1)\n    if dropout is not None:\n        attn = dropout(attn)\n    out = torch.matmul(attn, v)\n    return out, attn\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        self.d_model = d_model\n        self.max_len = max_len\n\n        # Precompute default positions up to max_len\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1).float()\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe.unsqueeze(0))  # (1, max_len, d_model)\n\n    def forward(self, x):\n        seq_len = x.size(1)\n        if seq_len > self.pe.size(1):\n            # 🔧 Dynamically expand positional encodings if sequence is longer\n            pe = torch.zeros(seq_len, self.d_model, device=x.device)\n            position = torch.arange(0, seq_len, device=x.device).unsqueeze(1)\n            div_term = torch.exp(torch.arange(0, self.d_model, 2, device=x.device) * (-math.log(10000.0) / self.d_model))\n            pe[:, 0::2] = torch.sin(position * div_term)\n            pe[:, 1::2] = torch.cos(position * div_term)\n            pe = pe.unsqueeze(0)\n            return x + pe\n        else:\n            return x + self.pe[:, :seq_len, :].to(x.device)\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads, dropout=0.1):\n        super().__init__()\n        assert d_model % num_heads == 0\n        self.num_heads = num_heads\n        self.depth = d_model // num_heads\n        self.wq = nn.Linear(d_model, d_model)\n        self.wk = nn.Linear(d_model, d_model)\n        self.wv = nn.Linear(d_model, d_model)\n        self.fc = nn.Linear(d_model, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def split_heads(self, x):\n        B, T, C = x.size()\n        return x.view(B, T, self.num_heads, self.depth).permute(0, 2, 1, 3)\n\n    def combine_heads(self, x):\n        x = x.permute(0, 2, 1, 3).contiguous()\n        B, T, _, _ = x.size()\n        return x.view(B, T, self.num_heads * self.depth)\n\n    def forward(self, q, k, v, mask=None):\n        q = self.split_heads(self.wq(q))\n        k = self.split_heads(self.wk(k))\n        v = self.split_heads(self.wv(v))\n        if mask is not None and mask.dtype != torch.bool:\n            mask = mask.bool()\n        out, attn = scaled_dot_product_attention(q, k, v, mask, dropout=self.dropout)\n        out = self.combine_heads(out)\n        return self.fc(out), attn\n\nclass FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(d_ff, d_model)\n\n    def forward(self, x):\n        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super().__init__()\n        self.mha = MultiHeadAttention(d_model, num_heads, dropout)\n        self.ffn = FeedForward(d_model, d_ff, dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask):\n        attn_out, _ = self.mha(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_out))\n        ffn_out = self.ffn(x)\n        x = self.norm2(x + self.dropout(ffn_out))\n        return x\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super().__init__()\n        self.self_mha = MultiHeadAttention(d_model, num_heads, dropout)\n        self.cross_mha = MultiHeadAttention(d_model, num_heads, dropout)\n        self.ffn = FeedForward(d_model, d_ff, dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, enc_out, tgt_mask, src_mask):\n        attn1, _ = self.self_mha(x, x, x, tgt_mask)\n        x = self.norm1(x + self.dropout(attn1))\n        attn2, _ = self.cross_mha(x, enc_out, enc_out, src_mask)\n        x = self.norm2(x + self.dropout(attn2))\n        ffn_out = self.ffn(x)\n        x = self.norm3(x + self.dropout(ffn_out))\n        return x\n\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model)\n        self.pos_enc = PositionalEncoding(d_model, max_len)\n        self.layers = nn.ModuleList([\n            EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n        ])\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, src, mask):\n        x = self.emb(src) * math.sqrt(self.emb.embedding_dim)\n        x = self.dropout(self.pos_enc(x))\n        for layer in self.layers:\n            x = layer(x, mask)\n        return x\n\nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model)\n        self.pos_enc = PositionalEncoding(d_model, max_len)\n        self.layers = nn.ModuleList([\n            DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n        ])\n        self.fc_out = nn.Linear(d_model, vocab_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, tgt, enc_out, tgt_mask, src_mask):\n        x = self.emb(tgt) * math.sqrt(self.emb.embedding_dim)\n        x = self.dropout(self.pos_enc(x))\n        for layer in self.layers:\n            x = layer(x, enc_out, tgt_mask, src_mask)\n        return self.fc_out(x)\n\n\nclass Transformer(nn.Module):\n    def __init__(self, vocab_size, d_model=256, num_heads=2, num_encoder_layers=2,\n                 num_decoder_layers=2, d_ff=1024, dropout=0.2, max_len=128, pad_id=0):\n        super().__init__()\n        self.encoder = Encoder(vocab_size, d_model, num_encoder_layers, num_heads, d_ff, dropout, max_len)\n        self.decoder = Decoder(vocab_size, d_model, num_decoder_layers, num_heads, d_ff, dropout, max_len)\n        self.pad_id = pad_id\n\n    def make_src_mask(self, src):\n        return create_padding_mask(src, self.pad_id)\n\n    def make_tgt_mask(self, tgt):\n        B, T = tgt.size()\n        pad_mask = create_padding_mask(tgt, self.pad_id)\n        look_ahead = create_look_ahead_mask(T).to(tgt.device)\n        look_ahead = look_ahead.unsqueeze(0).unsqueeze(1)\n        return pad_mask | look_ahead\n\n    def forward(self, src, tgt):\n        src_mask = self.make_src_mask(src)\n        tgt_mask = self.make_tgt_mask(tgt)\n        enc_out = self.encoder(src, src_mask)\n        logits = self.decoder(tgt, enc_out, tgt_mask, src_mask)\n        return logits\n\nif __name__ == \"__main__\":\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    vocab_size = tokenizer.get_vocab_size()\n\n    print(f\"✅ Device: {device}\")\n    print(f\"✅ Vocab Size: {vocab_size} | pad_id: {pad_id}\")\n\n    model = Transformer(\n        vocab_size=vocab_size,\n        d_model=256,\n        num_heads=2,\n        num_encoder_layers=2,\n        num_decoder_layers=2,\n        d_ff=1024,\n        dropout=0.2,\n        max_len=128,\n        pad_id=pad_id\n    ).to(device)\n\n    # Test one batch\n    sample = next(iter(train_loader))\n    src, tgt = sample[\"input_ids\"].to(device), sample[\"target_ids\"].to(device)\n    out = model(src, tgt)\n    print(\"✅ Forward pass successful — output shape:\", out.shape)\n    # Expected: (batch_size, seq_len, vocab_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T13:25:54.049015Z","iopub.execute_input":"2025-10-17T13:25:54.049588Z","iopub.status.idle":"2025-10-17T13:25:54.927476Z","shell.execute_reply.started":"2025-10-17T13:25:54.049563Z","shell.execute_reply":"2025-10-17T13:25:54.926839Z"}},"outputs":[{"name":"stdout","text":"✅ Device: cuda\n✅ Vocab Size: 2000 | pad_id: 0\n✅ Forward pass successful — output shape: torch.Size([32, 128, 2000])\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"pip install sacrebleu rouge-score --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T13:26:34.530870Z","iopub.execute_input":"2025-10-17T13:26:34.531127Z","iopub.status.idle":"2025-10-17T13:26:40.970958Z","shell.execute_reply.started":"2025-10-17T13:26:34.531109Z","shell.execute_reply":"2025-10-17T13:26:40.970017Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Task 4\n\nimport torch\nfrom torch.optim import Adam\nfrom tqdm import tqdm\nimport math\nimport sacrebleu\nfrom rouge_score import rouge_scorer\nimport torch.nn.functional as F\n\ndef compute_bleu(preds, refs):\n    return sacrebleu.corpus_bleu(preds, [refs]).score / 100\n\ndef compute_rougeL(preds, refs):\n    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n    scores = [scorer.score(r, p)['rougeL'].fmeasure for p, r in zip(preds, refs)]\n    return sum(scores)/len(scores) if scores else 0.0\n\ndef compute_chrf(preds, refs):\n    return sacrebleu.corpus_chrf(preds, [refs]).score\n\n\ndef generate(model, input_ids, tokenizer, max_len=60, device=\"cuda\"):\n    model.eval()\n    generated = input_ids.clone()\n    for _ in range(max_len):\n        with torch.no_grad():\n            outputs = model(generated, generated)\n            logits = outputs[:, -1, :]  # last token logits\n            next_token = logits.argmax(-1, keepdim=True)\n        generated = torch.cat((generated, next_token), dim=1)\n    return generated\n\n\ndef evaluate_model(model, val_loader, tokenizer, device, pad_id):\n    model.eval()\n    preds, refs = [], []\n    total_loss = 0\n    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=pad_id)\n\n    print(\"🔍 Starting full evaluation...\")\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n            input_ids = batch[\"input_ids\"].to(device)\n            target_ids = batch[\"target_ids\"].to(device)\n\n            logits = model(input_ids, target_ids[:, :-1])  # (B, T, V)\n            loss = loss_fn(\n                logits.reshape(-1, logits.size(-1)),\n                target_ids[:, 1:].reshape(-1)\n            )\n            total_loss += loss.item()\n\n            # Generate predictions\n            generated = generate(model, input_ids, tokenizer, max_len=60, device=device)\n            preds.extend([tokenizer.decode(g.tolist(), skip_special_tokens=True) for g in generated])\n            refs.extend([tokenizer.decode(r.tolist(), skip_special_tokens=True) for r in target_ids])\n\n    bleu = compute_bleu(preds, refs)\n    rouge = compute_rougeL(preds, refs)\n    chrf = compute_chrf(preds, refs)\n    ppl = math.exp(total_loss / len(val_loader))\n\n    print(f\"\\n📊 Validation — BLEU: {bleu:.4f} | ROUGE-L: {rouge:.4f} | chrF: {chrf:.4f} | PPL: {ppl:.2f}\")\n    return bleu, rouge, chrf, ppl\n\ndef train_loop(model, train_loader, val_loader, tokenizer, device, pad_id, epochs=10, lr=3e-4):\n    optimizer = Adam(model.parameters(), lr=lr, betas=(0.9, 0.98))\n    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=pad_id)\n    best_bleu = 0\n\n    print(f\"✅ GPU Device: {device}\\n\")\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n\n        for batch in pbar:\n            input_ids = batch[\"input_ids\"].to(device)\n            target_ids = batch[\"target_ids\"].to(device)\n\n            logits = model(input_ids, target_ids[:, :-1])\n            loss = loss_fn(\n                logits.reshape(-1, logits.size(-1)),\n                target_ids[:, 1:].reshape(-1)\n            )\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n            pbar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n\n        avg_loss = total_loss / len(train_loader)\n        print(f\"\\nEpoch {epoch+1} finished — Avg Train Loss: {avg_loss:.4f}\")\n\n        # Evaluate after each epoch\n        bleu, rouge, chrf, ppl = evaluate_model(model, val_loader, tokenizer, device, pad_id)\n\n        # Save best model\n        if bleu > best_bleu:\n            best_bleu = bleu\n            torch.save(model.state_dict(), \"best_model.pt\")\n            print(\"💾 Saved new best model!\\n\")\n\n    print(\"✅ Training complete! Best BLEU:\", best_bleu)\n\n# ===============================\n# 🚀 Run Training\n# ===============================\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\npad_id = tokenizer.token_to_id(\"<pad>\")\n\ntrain_loop(model, train_loader, val_loader, tokenizer, device, pad_id, epochs=20, lr=3e-4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T13:27:08.149484Z","iopub.execute_input":"2025-10-17T13:27:08.150135Z","iopub.status.idle":"2025-10-17T14:54:40.258951Z","shell.execute_reply.started":"2025-10-17T13:27:08.150108Z","shell.execute_reply":"2025-10-17T14:54:40.257535Z"}},"outputs":[{"name":"stdout","text":"✅ GPU Device: cuda\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/20: 100%|██████████| 1615/1615 [01:22<00:00, 19.48it/s, Loss=4.0902]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1 finished — Avg Train Loss: 4.6082\n🔍 Starting full evaluation...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 202/202 [04:50<00:00,  1.44s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Validation — BLEU: 0.0138 | ROUGE-L: 0.0911 | chrF: 18.2817 | PPL: 62.36\n💾 Saved new best model!\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/20: 100%|██████████| 1615/1615 [01:27<00:00, 18.35it/s, Loss=3.8686]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2 finished — Avg Train Loss: 4.1109\n🔍 Starting full evaluation...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 202/202 [04:48<00:00,  1.43s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Validation — BLEU: 0.0141 | ROUGE-L: 0.0951 | chrF: 18.2842 | PPL: 52.12\n💾 Saved new best model!\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/20: 100%|██████████| 1615/1615 [01:27<00:00, 18.44it/s, Loss=4.1763]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3 finished — Avg Train Loss: 3.9631\n🔍 Starting full evaluation...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 202/202 [04:48<00:00,  1.43s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Validation — BLEU: 0.0141 | ROUGE-L: 0.0938 | chrF: 18.7262 | PPL: 47.45\n💾 Saved new best model!\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/20: 100%|██████████| 1615/1615 [01:27<00:00, 18.44it/s, Loss=3.9047]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4 finished — Avg Train Loss: 3.8758\n🔍 Starting full evaluation...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 202/202 [04:48<00:00,  1.43s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Validation — BLEU: 0.0145 | ROUGE-L: 0.0978 | chrF: 18.8150 | PPL: 44.65\n💾 Saved new best model!\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/20: 100%|██████████| 1615/1615 [01:27<00:00, 18.46it/s, Loss=3.8241]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5 finished — Avg Train Loss: 3.8137\n🔍 Starting full evaluation...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 202/202 [04:48<00:00,  1.43s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Validation — BLEU: 0.0145 | ROUGE-L: 0.0977 | chrF: 18.8217 | PPL: 42.47\n💾 Saved new best model!\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/20: 100%|██████████| 1615/1615 [01:27<00:00, 18.36it/s, Loss=4.0270]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6 finished — Avg Train Loss: 3.7657\n🔍 Starting full evaluation...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 202/202 [04:47<00:00,  1.42s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Validation — BLEU: 0.0144 | ROUGE-L: 0.0966 | chrF: 18.7836 | PPL: 41.00\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/20: 100%|██████████| 1615/1615 [01:27<00:00, 18.36it/s, Loss=3.7344]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7 finished — Avg Train Loss: 3.7256\n🔍 Starting full evaluation...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 202/202 [04:47<00:00,  1.42s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Validation — BLEU: 0.0143 | ROUGE-L: 0.0950 | chrF: 18.7327 | PPL: 39.98\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/20: 100%|██████████| 1615/1615 [01:27<00:00, 18.38it/s, Loss=3.6357]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 8 finished — Avg Train Loss: 3.6937\n🔍 Starting full evaluation...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 202/202 [04:48<00:00,  1.43s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Validation — BLEU: 0.0143 | ROUGE-L: 0.0948 | chrF: 18.6489 | PPL: 38.90\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/20: 100%|██████████| 1615/1615 [01:27<00:00, 18.37it/s, Loss=3.8944]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9 finished — Avg Train Loss: 3.6633\n🔍 Starting full evaluation...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 202/202 [04:48<00:00,  1.43s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Validation — BLEU: 0.0145 | ROUGE-L: 0.0970 | chrF: 18.7969 | PPL: 38.25\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/20: 100%|██████████| 1615/1615 [01:27<00:00, 18.43it/s, Loss=3.7251]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10 finished — Avg Train Loss: 3.6382\n🔍 Starting full evaluation...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 202/202 [04:47<00:00,  1.42s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Validation — BLEU: 0.0141 | ROUGE-L: 0.0924 | chrF: 18.5947 | PPL: 37.76\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/20: 100%|██████████| 1615/1615 [01:27<00:00, 18.41it/s, Loss=3.7140]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 11 finished — Avg Train Loss: 3.6138\n🔍 Starting full evaluation...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 202/202 [04:47<00:00,  1.42s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Validation — BLEU: 0.0139 | ROUGE-L: 0.0893 | chrF: 18.5948 | PPL: 37.49\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/20: 100%|██████████| 1615/1615 [01:27<00:00, 18.42it/s, Loss=3.7937]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 12 finished — Avg Train Loss: 3.5944\n🔍 Starting full evaluation...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 202/202 [04:47<00:00,  1.42s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Validation — BLEU: 0.0142 | ROUGE-L: 0.0929 | chrF: 18.6881 | PPL: 36.72\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/20: 100%|██████████| 1615/1615 [01:27<00:00, 18.45it/s, Loss=3.5537]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 13 finished — Avg Train Loss: 3.5776\n🔍 Starting full evaluation...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 202/202 [04:47<00:00,  1.42s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Validation — BLEU: 0.0144 | ROUGE-L: 0.0959 | chrF: 18.6792 | PPL: 36.24\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/20: 100%|██████████| 1615/1615 [01:27<00:00, 18.46it/s, Loss=3.6006]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 14 finished — Avg Train Loss: 3.5590\n🔍 Starting full evaluation...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  58%|█████▊    | 117/202 [02:47<02:01,  1.43s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/1795555269.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0mpad_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_to_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<pad>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_37/1795555269.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(model, train_loader, val_loader, tokenizer, device, pad_id, epochs, lr)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# Evaluate after each epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mbleu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrouge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchrf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mppl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# Save best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_37/1795555269.py\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, val_loader, tokenizer, device, pad_id)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;31m# Generate predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerated\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mrefs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_37/1795555269.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(model, input_ids, tokenizer, max_len, device)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# last token logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mnext_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_37/3590621887.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0msrc_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_src_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0mtgt_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_tgt_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0menc_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_37/3590621887.py\u001b[0m in \u001b[0;36mmake_tgt_mask\u001b[0;34m(self, tgt)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mpad_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_padding_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mlook_ahead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_look_ahead_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0mlook_ahead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlook_ahead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpad_mask\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mlook_ahead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":5},{"cell_type":"code","source":"# Task 5\nimport random\n\nmodel.load_state_dict(torch.load(\"best_model.pt\", map_location=device))\nmodel.to(device)\nmodel.eval()\n\npad_id = tokenizer.token_to_id(\"<pad>\")\nbleu, rouge, chrf, ppl = evaluate_model(model, val_loader, tokenizer, device, pad_id)\n\n\nprint(\"\\n📈 Automatic Evaluation Results:\")\nprint(f\"BLEU: {bleu:.4f}\")\nprint(f\"ROUGE-L: {rouge:.4f}\")\nprint(f\"chrF: {chrf:.4f}\")\nprint(f\"Perplexity: {ppl:.2f}\")\n\nprint(\"\\n🔍 Sample Qualitative Outputs (Human Evaluation):\")\nnum_examples = 3\nbatches = [next(iter(val_loader)) for _ in range(num_examples)]\n\nfor i, batch in enumerate(batches, 1):\n    input_ids = batch[\"input_ids\"].to(device)\n    target_ids = batch[\"target_ids\"].to(device)\n\n    with torch.no_grad():\n        generated = generate(model, input_ids, tokenizer, max_len=60, device=device)\n\n    input_text = tokenizer.decode(input_ids[0].tolist(), skip_special_tokens=True)\n    pred_text = tokenizer.decode(generated[0].tolist(), skip_special_tokens=True)\n    ref_text = tokenizer.decode(target_ids[0].tolist(), skip_special_tokens=True)\n\n    print(f\"\\nExample {i}\")\n    print(f\"📝 Input:     {input_text[:200]}...\")\n    print(f\"🤖 Predicted: {pred_text[:200]}...\")\n    print(f\"🎯 Reference: {ref_text[:200]}...\")\n    print(\"💬 Human Ratings — Fluency: __ | Relevance: __ | Adequacy: __\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T14:57:11.826880Z","iopub.execute_input":"2025-10-17T14:57:11.827425Z","iopub.status.idle":"2025-10-17T15:02:15.071016Z","shell.execute_reply.started":"2025-10-17T14:57:11.827400Z","shell.execute_reply":"2025-10-17T15:02:15.069686Z"}},"outputs":[{"name":"stdout","text":"🔍 Starting full evaluation...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 202/202 [04:50<00:00,  1.44s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Validation — BLEU: 0.0145 | ROUGE-L: 0.0977 | chrF: 18.8217 | PPL: 42.47\n\n📈 Automatic Evaluation Results:\nBLEU: 0.0145\nROUGE-L: 0.0977\nchrF: 18.8217\nPerplexity: 42.47\n\n🔍 Sample Qualitative Outputs (Human Evaluation):\n\nExample 1\n📝 Input:     emotion excited situation i got off work at am today ! time to head to the beach . customer customer i got off work at am today ! time to head to the beach . agent agent...\n🤖 Predicted: emotion excited situation i got off work at am today ! time to head to the beach . customer customer i got off work at am today ! time to head to the beach . agent agent . . . . . . . . . . . . . . . ...\n🎯 Reference: ah h h h h h h h h h ! that is amazing . i am so jealous of you . where are you going ?...\n💬 Human Ratings — Fluency: __ | Relevance: __ | Adequacy: __\n\nExample 2\n📝 Input:     emotion excited situation i got off work at am today ! time to head to the beach . customer customer i got off work at am today ! time to head to the beach . agent agent...\n🤖 Predicted: emotion excited situation i got off work at am today ! time to head to the beach . customer customer i got off work at am today ! time to head to the beach . agent agent . . . . . . . . . . . . . . . ...\n🎯 Reference: ah h h h h h h h h h ! that is amazing . i am so jealous of you . where are you going ?...\n💬 Human Ratings — Fluency: __ | Relevance: __ | Adequacy: __\n\nExample 3\n📝 Input:     emotion excited situation i got off work at am today ! time to head to the beach . customer customer i got off work at am today ! time to head to the beach . agent agent...\n🤖 Predicted: emotion excited situation i got off work at am today ! time to head to the beach . customer customer i got off work at am today ! time to head to the beach . agent agent . . . . . . . . . . . . . . . ...\n🎯 Reference: ah h h h h h h h h h ! that is amazing . i am so jealous of you . where are you going ?...\n💬 Human Ratings — Fluency: __ | Relevance: __ | Adequacy: __\n","output_type":"stream"}],"execution_count":8}]}